{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f0269d-3aab-44fc-9ffc-c38de6fc6b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!uv pip install -q nltk seaborn pandas WordCloud unidecode scikit-learn pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48680fd8-a754-4ae1-b2bb-9d544f334eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot  as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re \n",
    "import seaborn as sns\n",
    "import string\n",
    "import unidecode\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0f7da8-eaf3-462f-86ce-a18cecced6e4",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d305d4-0c93-4051-a611-99726c7d95cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DICTIONNARY =  ['accord','entreprise', 'preambule', 'sommaire',  'code', 'syndical', 'responsable', 'representant', \n",
    "                'present', 'ca', 'organisation', 'preambule', 'peut', 'etre', 'contrat','travail', 'ressources','humaines', 'mise',\n",
    "                'ainsi', 'et', 'ou', 'alors','collaborateur', 'ci', 'apres', 'party', 'signataire', 'tout', 'etat', 'cause', 'societe', \n",
    "                'notamment','article','activite', 'cette', 'donc', 'si', 'sous', 'disposition', 'convention', 'collective', 'dans', 'a', 'cadre', \n",
    "                'signataire', 'partie', 'parties', 'entre', 'doit'\n",
    "               ]\n",
    "\n",
    "DICTIONNARY_STEM = ['part', 'signatair', 'organis', 'syndical', \n",
    "                    'dont', 'sieg', 'social', 'conseil', 'prud', 'homm', \n",
    "                   'vi', 'professionnel', 'disposit', 'legal', 'conventionnel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cba7eb-7c98-4e91-b0da-a9277c40f850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(path):\n",
    "    \"\"\"\n",
    "    Load a DataFrame from a parquet file.\n",
    "\n",
    "    Parameters:\n",
    "    path (str): The file path to the parquet file.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The loaded pandas DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(path)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa252f2e-ff12-430a-ac7a-f3f616b7b856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text, lang=\"french\"):\n",
    "    \"\"\"\n",
    "    Tokenize the input text, remove stopwords, and apply stemming.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to process.\n",
    "    lang (str): The language for stemming (default is \"french\").\n",
    "\n",
    "    Returns:\n",
    "    list: List of processed words (tokenized, cleaned, and stemmed).\n",
    "    \"\"\"\n",
    "    stemmer = SnowballStemmer(lang)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words_preprocessed = [unidecode.unidecode(w.lower()) for w in words \n",
    "                          if w.lower() not in stop_words \n",
    "                          and w not in string.punctuation \n",
    "                          and unidecode.unidecode(w.lower()) not in DICTIONNARY \n",
    "                          and w not in (\"’\",\"__\",\"--\") \n",
    "                          and not re.search(r'xx+', w.lower()) \n",
    "                          and not re.search(r'_+', w.lower()) \n",
    "                          and not re.search(r'-+', w.lower())  \n",
    "                         ]\n",
    "\n",
    "    words_processed = [stemmer.stem(word) for word in words_preprocessed\n",
    "                      if stemmer.stem(word) not in DICTIONNARY_STEM ]\n",
    "    return words_processed\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_text(text, lang=\"french\"):\n",
    "    stemmer = SnowballStemmer(lang)\n",
    "    words = word_tokenize(text)\n",
    "    words_preprocessed = [\n",
    "        unidecode.unidecode(w.lower()) for w in words\n",
    "        if w.lower() not in stop_words\n",
    "        and unidecode.unidecode(w.lower()) not in DICTIONNARY  \n",
    "        and w not in (\"’\", \"__\", \"--\")\n",
    "        and w not in string.punctuation\n",
    "        and not re.search(r'xx+|_+|-+', w.lower())\n",
    "    ]\n",
    "    # Stemming des mots prétraités\n",
    "    return [stemmer.stem(word) for word in words_preprocessed]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f467897c-a387-40b3-8792-5dd30d09f5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common(words, n=10):\n",
    "    \"\"\"\n",
    "    Return the n most common words or n-grams from a list.\n",
    "\n",
    "    Parameters:\n",
    "    words (list): List of words or n-grams.\n",
    "    n (int): The number of most common items to return (default is 10).\n",
    "\n",
    "    Returns:\n",
    "    list: List of the most common n-grams or words.\n",
    "    \"\"\"    \n",
    "    fdist = FreqDist(words)\n",
    "    return [ngram for ngram, _ in fdist.most_common(n)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e12c7d1-da9d-48fb-b1f7-5f51ec6fccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud(words):\n",
    "    \"\"\"\n",
    "    Generate and display a word cloud based on a list of words.\n",
    "\n",
    "    Parameters:\n",
    "    words (list): List of words used to create the word cloud.\n",
    "\n",
    "    Returns:\n",
    "    None: The function directly displays the word cloud.\n",
    "    \"\"\"    \n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(\" \".join(words))  # Créer le nuage de mots\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7a6a62-11ef-425c-ac0e-9f279240b3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngram(tokens,n=2):\n",
    "    \"\"\"\n",
    "    Generate n-grams from a list of tokens.\n",
    "\n",
    "    Parameters:\n",
    "    tokens (list): List of tokens (words) from a text.\n",
    "    n (int): The size of the n-grams (default is 2 for bigrams).\n",
    "\n",
    "    Returns:\n",
    "    list: List of n-grams generated from the tokens.\n",
    "    \"\"\"\n",
    "    # Generate ngrams\n",
    "    ngram = list(ngrams(tokens, n))\n",
    "    return ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac86d7a-e250-4a63-9f77-d7671b60b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top10ngrams(tokens, n):\n",
    "    \"\"\"\n",
    "    Plots the top 10 n-grams with the highest frequencies from a list of tokens.\n",
    "\n",
    "    Parameters:\n",
    "    tokens (list): List of tokens (words) to generate n-grams from.\n",
    "    n (int): The size of the n-grams (e.g., 2 for bigrams, 3 for trigrams).\n",
    "\n",
    "    Returns:\n",
    "    None: The function directly displays a bar plot showing the top 10 most frequent n-grams.\n",
    "    \"\"\"\n",
    "    # Generate n-grams\n",
    "    ngrams = generate_ngram(tokens,n)\n",
    "    # Frequency distribution of n-grams\n",
    "    fdist = FreqDist(ngrams)\n",
    "    # Convert the frequency distribution to a pandas DataFrame\n",
    "    ngram_freq_df = pd.DataFrame(fdist.items(), columns=['Ngram', 'Frequency'])\n",
    "    \n",
    "    # Split the ngram tuples into separate columns for better handling\n",
    "    ngram_freq_df[['Word' + str(i+1) for i in range(n)]] = pd.DataFrame(ngram_freq_df['Ngram'].tolist(), index=ngram_freq_df.index)\n",
    "    \n",
    "    # Drop the original 'Ngram' column as it's no longer needed\n",
    "    ngram_freq_df.drop(columns=['Ngram'], inplace=True)\n",
    "    \n",
    "    # Sort by frequency to get the most common ngrams\n",
    "    ngram_freq_df = ngram_freq_df.sort_values(by='Frequency', ascending=False)\n",
    "    \n",
    "    # Create a new column for better labeling in the plot (e.g., 'Word1 Word2')\n",
    "    ngram_freq_df['Ngram_label'] = ngram_freq_df[['Word' + str(i+1) for i in range(n)]].agg(' '.join, axis=1)\n",
    "    \n",
    "    # Plot the top 10 ngrams using Seaborn\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Frequency', y='Ngram_label', data=ngram_freq_df.head(10), hue='Ngram_label', palette='viridis', dodge=False, legend=False)\n",
    "    plt.title(f'Top 10 {n}-grams by Frequency')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel(f'{n}-grams')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8b24d8-71f7-4f8c-8a3f-92fccca0d6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngrams_absolute(text_tokens, top_ngrams, n):\n",
    "    \"\"\"\n",
    "    Count the absolute frequency of each top n-gram in a list of tokens.\n",
    "\n",
    "    Parameters:\n",
    "    text_tokens (list): List of tokens from a text.\n",
    "    top_ngrams (list): List of the most common n-grams to check.\n",
    "    n (int): The size of the n-grams (e.g., 2 for bigrams).\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with n-grams as keys and their absolute frequencies as values.\n",
    "    \"\"\"\n",
    "    text_ngrams = list(ngrams(text_tokens, n))\n",
    "    return {ngram: text_ngrams.count(ngram) for ngram in top_ngrams}\n",
    "\n",
    "# Fonction pour tracer les 10 n-grammes les plus fréquents en fréquence absolue\n",
    "def plot_top10ngrams_abs_freq(df, tokens, n):\n",
    "    \"\"\"\n",
    "    Plots the top 10 n-grams with the mean of their absolute frequency in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the tokens for each document.\n",
    "    tokens (list): List of tokens used to generate n-grams.\n",
    "    n (int): Size of the n-grams (e.g., 2 for bigrams).\n",
    "\n",
    "    Returns:\n",
    "    None: The function displays a bar plot of the top 10 n-grams with their frequencies.\n",
    "    \"\"\"    \n",
    "    ngrams_list = generate_ngram(tokens, n)\n",
    "    top_ngrams = most_common(ngrams_list, 10)\n",
    "    \n",
    "    # Calcul des fréquences absolues des n-grammes pour chaque document\n",
    "    df[f\"{n}gram_counts_abs\"] = df[\"tokens\"].apply(lambda tokens: count_ngrams_absolute(tokens, top_ngrams, n))\n",
    "    \n",
    "    # Convertir en DataFrame\n",
    "    ngram_counts_df = pd.DataFrame(df[f\"{n}gram_counts_abs\"].tolist(), index=df.index)\n",
    "    \n",
    "    # Calcul de la moyenne des fréquences absolues\n",
    "    ngram_means = ngram_counts_df.mean()\n",
    "    ngram_means = ngram_means.sort_values(ascending=False)\n",
    "    \n",
    "    # Tracé du graphique\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=ngram_means.values, \n",
    "                y=[\" \".join(map(str, ngram)) for ngram in ngram_means.index], \n",
    "                hue=[\" \".join(map(str, ngram)) for ngram in ngram_means.index],  \n",
    "                palette=\"viridis\", \n",
    "                legend=False)\n",
    "    \n",
    "    plt.xlabel(\"Fréquence absolue moyenne\")\n",
    "    plt.ylabel(f\"{n}-grammes\")\n",
    "    plt.title(f\"Moyenne de la fréquence absolue des {len(top_ngrams)} {n}-grammes les plus fréquents\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048d0d7d-43a6-4329-92aa-5d4e4ef75859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour compter la fréquence relative des n-grammes\n",
    "def count_ngrams_relative(text_tokens, top_ngrams, n):\n",
    "    \"\"\"\n",
    "    Counts the relative frequency of specified n-grams in a list of tokens.\n",
    "\n",
    "    Parameters:\n",
    "    text_tokens (list): List of tokens (words) from a text.\n",
    "    top_ngrams (list): List of n-grams for which to compute the relative frequency.\n",
    "    n (int): Size of the n-grams (e.g., 2 for bigrams, 3 for trigrams).\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are the top n-grams and values are their relative frequencies in the text.\n",
    "    \"\"\"\n",
    "    text_ngrams = list(ngrams(text_tokens, n))\n",
    "    num_words = len(text_tokens)  # Nombre total de mots\n",
    "    if num_words == 0:\n",
    "        return {ngram: 0 for ngram in top_ngrams}  \n",
    "    return {ngram: text_ngrams.count(ngram) / num_words for ngram in top_ngrams}\n",
    "\n",
    "# Fonction pour tracer les 10 n-grammes les plus fréquents\n",
    "def plot_top10ngrams_relative_freq(df, tokens, n):\n",
    "    \"\"\"\n",
    "    Plots the top 10 n-grams with the mean relative frequency from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): A DataFrame with a 'tokens' column, where each row contains a list of tokens.\n",
    "    tokens (list): List of tokens from which to generate n-grams.\n",
    "    n (int): Size of the n-grams (e.g., 2 for bigrams, 3 for trigrams).\n",
    "\n",
    "    Returns:\n",
    "    None: Displays a bar plot of the mean relative frequency of the top 10 n-grams.\n",
    "    \"\"\"    \n",
    "    ngrams_list = generate_ngram(tokens, n)\n",
    "    top_ngrams = most_common(ngrams_list, 10)\n",
    "    \n",
    "    # Calcul des fréquences des n-grammes pour chaque document\n",
    "    df[f\"{n}gram_counts_relative\"] = df[\"tokens\"].apply(lambda tokens: count_ngrams_relative(tokens, top_ngrams, n))\n",
    "    \n",
    "    # Convertir en DataFrame\n",
    "    ngram_counts_df = pd.DataFrame(df[f\"{n}gram_counts_relative\"].tolist(), index=df.index)\n",
    "    \n",
    "    # Calcul de la moyenne des fréquences relatives\n",
    "    ngram_means = ngram_counts_df.mean()\n",
    "    ngram_means = ngram_means.sort_values(ascending=False)\n",
    "    \n",
    "    # Tracé du graphique\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=ngram_means.values, \n",
    "                y=[\" \".join(map(str, ngram)) for ngram in ngram_means.index], \n",
    "                hue=[\" \".join(map(str, ngram)) for ngram in ngram_means.index],  \n",
    "                palette=\"viridis\", \n",
    "                legend=False)\n",
    "    \n",
    "    plt.xlabel(\"Fréquence relative\")\n",
    "    plt.ylabel(f\"{n}-grammes\")\n",
    "    plt.title(f\"Moyenne de la fréquence relative des {len(top_ngrams)} {n}-grammes les plus fréquents\")\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da41e9fd-9bd6-40e6-a885-e69082938a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngrams(text_tokens, n):\n",
    "    if not isinstance(text_tokens, list):  \n",
    "        return []\n",
    "    return list(ngrams(text_tokens, n))\n",
    "\n",
    "    \n",
    "# Fonction pour générer les trigrammes\n",
    "def count_specific_ngram_relative(tokens, target_ngram,n):\n",
    "    ngrams = count_ngrams(tokens, n)\n",
    "    num_words = len(tokens)\n",
    "    if num_words == 0 :\n",
    "        return 0\n",
    "    else : \n",
    "        return (ngrams.count(target_ngram)/ num_words)\n",
    "\n",
    "\n",
    "def count_specific_ngram_abs(tokens, target_ngram,n):\n",
    "    ngrams = count_ngrams(tokens, n)\n",
    "    return (ngrams.count(target_ngram))\n",
    "\n",
    "def plot_distribution(df,target_ngram,count,n):\n",
    "    \"\"\"\n",
    "    Plots the distribution of the frequency of a specific n-gram in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing a 'tokens' column, where each row is a list of tokens (words).\n",
    "    target_ngram (tuple): The specific n-gram (e.g., a trigram) to track the frequency of.\n",
    "    count (function): The function that counts the occurrences of a given n-gram in a list of tokens \n",
    "    (can be relative or absolute) \n",
    "    n (int): The size of the n-grams (e.g., 2 for bigrams, 3 for trigrams).\n",
    "\n",
    "    Returns:\n",
    "    None: The function directly displays a bar plot of the frequency distribution of the target n-gram.\n",
    "    \"\"\"\n",
    "    # Vérifier si la colonne 'tokens' existe et ne contient pas de valeurs NaN\n",
    "    if 'tokens' in df.columns and df['tokens'].notna().all():\n",
    "        # Appliquer la fonction pour compter le trigramme spécifique\n",
    "        df[\"target_ngram_freq\"] = df[\"tokens\"].apply(lambda tokens: count(tokens, target_ngram,n))\n",
    "    \n",
    "        # Trier les fréquences dans l'ordre décroissant\n",
    "        freq_values = df[\"target_ngram_freq\"].sort_values(ascending=False).reset_index(drop=True)\n",
    "    \n",
    "        # Vérifier si au moins un texte contient le trigramme\n",
    "        if freq_values.sum() == 0:\n",
    "            print(f\"Aucun ngramme {target_ngram} trouvé dans les textes.\")\n",
    "        else:\n",
    "            # Tracer la distribution des fréquences\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.barplot(x=freq_values.index, y=freq_values, hue=freq_values.index, palette=\"viridis\", legend=False)\n",
    "            \n",
    "            # Supprimer l'axe des abscisses\n",
    "            plt.xticks([])  # Supprime les ticks sur l'axe x\n",
    "            plt.xlabel('')   # Supprime le titre de l'axe x\n",
    "            \n",
    "            # Ajouter un titre et un label pour l'axe des ordonnées\n",
    "            plt.title(f\"Distribution du ngramme {target_ngram}\")\n",
    "            plt.ylabel(f\"Fréquence du ngramme {target_ngram}\")\n",
    "            \n",
    "            # Afficher le graphique\n",
    "            plt.show()\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        print(\"La colonne 'tokens' est absente ou contient des valeurs NaN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90beb48f-53b4-42f0-b588-29a96635f7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df_path):\n",
    "    df = load_df(df_path)\n",
    "    df[\"tokens\"] = df[\"accorddocx\"].apply(preprocess_text)\n",
    "    df['text_length'] = df['accorddocx'].apply(len)\n",
    "    TEXT=\"\".join(df.accorddocx)\n",
    "    words_processed = process_text(TEXT)\n",
    "    return df, words_processed \n",
    "\n",
    "def analyse_dataframe(df_path):\n",
    "    df, words_processed = process_dataframe(df_path)\n",
    "    wordcloud(words_processed)\n",
    "    n_gram_sizes = [2, 3, 4]\n",
    "    for n in n_gram_sizes:\n",
    "    # Génération des n-grams\n",
    "        n_grams = generate_ngram(words_processed, n)\n",
    "        top_n_grams = most_common(n_grams, 1)\n",
    "        top_n_gram = top_n_grams[0]\n",
    "        \n",
    "    # Affichage des 10 n-grams les plus fréquents\n",
    "        plot_top10ngrams(words_processed, n)\n",
    "        plot_top10ngrams_abs_freq(df, words_processed, n)\n",
    "        plot_top10ngrams_relative_freq(df, words_processed, n)\n",
    "\n",
    "        plot_distribution(df, top_n_gram, count_specific_ngram_abs, n)  # Distribution absolue\n",
    "        plot_distribution(df, top_n_gram, count_specific_ngram_relative, n)  # Distribution relative\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11716786-ed20-4d56-b39d-8854c3adc975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngrams_abs(text_tokens, ngrams_to_watch, n):\n",
    "    text_ngrams = list(ngrams(text_tokens, n))  #création des n-gram possibles avec la liste de tokens\n",
    "    ngram_counts = Counter(text_ngrams) #compte le nb de fois où chaque ngram apparait dans le texte\n",
    "    return {ngram: ngram_counts.get(ngram, 0) for ngram in ngrams_to_watch} # ngram et sa fréquece relative d'apparition\n",
    "\n",
    "def classify_hs_abs(text_tokens, ngram_thresholds_hs):\n",
    "    \"\"\"\n",
    "    Classifies the text based on the provided n-grams and thresholds.\n",
    "    If a threshold is exceeded, the text is classified under that category.\n",
    "    Returns binary values (1 or 0) for the columns 'HS' and 'non_classified'.\n",
    "\n",
    "    Parameters:\n",
    "    text_tokens (list): A list of tokens (words) from the text to be classified.\n",
    "    ngram_thresholds_hs (dict): A dictionary where keys are labels ('heures supp', etc.), and values are lists of \n",
    "                                 tuples, each containing an n-gram and a threshold value.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two binary values:\n",
    "           - `HS` (1 if the text belongs to 'heures supp', 0 otherwise)\n",
    "           - `non_classified` (1 if the text is not classified, 0 otherwise)\n",
    "    \"\"\"\n",
    "    # Initialize the columns 'HS' and 'non_classified' to 0\n",
    "    hs = 0\n",
    "    non_classified = 1  # By default, the text is classified as non-classified\n",
    "\n",
    "    # Loop through the n-grams and thresholds defined in ngram_thresholds_hs\n",
    "    for label, ngrams in ngram_thresholds_hs.items():\n",
    "        for ngram, threshold in ngrams:\n",
    "            ngram_freq = count_ngrams_abs(text_tokens, [ngram], len(ngram)).get(ngram, 0)\n",
    "            \n",
    "            # If the frequency of the n-gram exceeds the threshold, classify as 'heures supp' (HS)\n",
    "            if ngram_freq >= threshold:\n",
    "                hs = 1\n",
    "                non_classified = 0\n",
    "                break  # Exit the loop as soon as one threshold is exceeded\n",
    "\n",
    "    # Return binary values\n",
    "    return hs, non_classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82419109-58d2-4a18-8766-c59ad3a8ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngrams_relative(text_tokens, ngrams_to_watch, n):\n",
    "    text_ngrams = list(ngrams(text_tokens, n))  #création des n-gram possibles avec la liste de tokens\n",
    "    num_words = len(text_tokens) #longueur texte\n",
    "    if num_words == 0:\n",
    "        return {ngram: 0 for ngram in ngrams_to_watch}\n",
    "    ngram_counts = Counter(text_ngrams) #compte le nb de fois où chaque ngram apparait dans le texte\n",
    "    return {ngram: ngram_counts.get(ngram, 0) / num_words for ngram in ngrams_to_watch} # ngram et sa fréquece relative d'apparition\n",
    "\n",
    "def classify_hs_relative(text_tokens, ngram_thresholds_hs):\n",
    "    \"\"\"\n",
    "    Classifies the text based on the provided n-grams and thresholds.\n",
    "    If a threshold is exceeded, the text is classified under that category.\n",
    "    Returns binary values (1 or 0) for the columns 'HS' and 'non_classified'.\n",
    "\n",
    "    Parameters:\n",
    "    text_tokens (list): A list of tokens (words) from the text to be classified.\n",
    "    ngram_thresholds_hs (dict): A dictionary where keys are labels ('heures supp', etc.), and values are lists of \n",
    "                                 tuples, each containing an n-gram and a threshold value.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two binary values:\n",
    "           - `HS` (1 if the text belongs to 'heures supp', 0 otherwise)\n",
    "           - `non_classified` (1 if the text is not classified, 0 otherwise)\n",
    "    \"\"\"\n",
    "    # Initialize the columns 'HS' and 'non_classified' to 0\n",
    "    hs = 0\n",
    "    non_classified = 1  # By default, the text is classified as non-classified\n",
    "\n",
    "    # Loop through the n-grams and thresholds defined in ngram_thresholds_hs\n",
    "    for label, ngrams in ngram_thresholds_hs.items():\n",
    "        for ngram, threshold in ngrams:\n",
    "            ngram_freq = count_ngrams_relative(text_tokens, [ngram], len(ngram)).get(ngram, 0)\n",
    "            \n",
    "            # If the frequency of the n-gram exceeds the threshold, classify as 'heures supp' (HS)\n",
    "            if ngram_freq >= threshold:\n",
    "                hs = 1\n",
    "                non_classified = 0\n",
    "                break  # Exit the loop as soon as one threshold is exceeded\n",
    "\n",
    "    # Return binary values\n",
    "    return hs, non_classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff7ba59-4724-47d6-985c-089d945d866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conf_matrix(df, target, classif_function, ngram_thresholds):\n",
    "    \"\"\"\n",
    "    Plots the confusion matrix and classification report for the predicted labels.\n",
    "    \n",
    "    This function uses a classification function to predict labels for the text tokens in the DataFrame\n",
    "    and compares them to the actual labels to generate the confusion matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the 'tokens' column (list of words/tokens) and a target column (true labels).\n",
    "    target (str): The column name in the DataFrame which contains the true labels.\n",
    "    classif_function (function): The classification function to apply to each row of 'tokens'.\n",
    "                                  It should return a tuple of binary values for the columns.\n",
    "    ngram_thresholds (dict): A dictionary containing the n-grams and thresholds used by the classification function.\n",
    "    \n",
    "    Returns:\n",
    "    None: Displays a heatmap of the confusion matrix and prints a classification report.\n",
    "    \"\"\"\n",
    "    # Apply the classification function to the tokens column and predict labels\n",
    "    df[f'predicted_labels_{target}'] = df['tokens'].apply(lambda tokens: classif_function(tokens, ngram_thresholds))\n",
    "    \n",
    "    # True labels from the DataFrame\n",
    "    y_true = df[target]\n",
    "    \n",
    "    # Predicted labels from the classification function\n",
    "    # If the classification function returns a tuple, we take the first element (e.g., binary classification)\n",
    "    y_pred = df[f'predicted_labels_{target}'].apply(lambda labels: labels[0] if len(labels) > 0 else 0)  # Adjust for your label format\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred, labels=[1, 0])  # 1 for the relevant class, 0 for non-relevant class\n",
    "\n",
    "    # Display confusion matrix as a heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=[f'{target}', 'non classifié'], \n",
    "                yticklabels=[f'{target}', 'non classifié'])\n",
    "    plt.xlabel('Predictions')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title(f'Confusion Matrix for {target}')\n",
    "    plt.show()\n",
    "    \n",
    "    # Display the classification report\n",
    "    print(classification_report(y_true, y_pred, target_names=[f'{target}', 'non classifié'], zero_division=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f701a4e-0d8b-478c-9e2b-1e24a484c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(df_classif_path, thematique, ngram_thresholds_relative, ngram_thresholds_abs):\n",
    "    df_classif, words_processed_classif= process_dataframe(df_classif_path)\n",
    "    df_classif[['HS', 'TT', 'CET']] = df_classif[['HS', 'TT', 'CET']].astype(int)\n",
    "    print(\"Classification avec fréquence relative\")\n",
    "    plot_conf_matrix(df_classif,thematique,classify_hs_relative, ngram_thresholds_relative)\n",
    "    print(\"_\" * 100)\n",
    "    print(\"Classification avec fréquence absolue\")\n",
    "    plot_conf_matrix(df_classif,thematique,classify_hs_abs, ngram_thresholds_abs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5833188-e570-48b6-be47-f3cb0a02baa5",
   "metadata": {},
   "source": [
    "## Exemple d'utilisation HS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01feb50d-e693-4878-8370-1245f596c65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de df_hs\n",
    "analyse_dataframe(\"data/echantillon_1000_hs_accords.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6389c1d8-3ae2-494d-b12a-10d26036e38a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ngram_thresholds_hs_relative = {\n",
    "    'HS': [\n",
    "        (('heur', 'supplementair'), 0.002)\n",
    "    ]\n",
    "}\n",
    "\n",
    "ngram_thresholds_hs_abs = {\n",
    "    'HS': [\n",
    "        (('heur', 'supplementair'), 7)\n",
    "    ]\n",
    "} \n",
    "\n",
    "classification(\"data/echantillon_hs_tt_cet_accords_et_thematiques.parquet\", 'HS',ngram_thresholds_hs_relative,ngram_thresholds_hs_abs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eff11a-699d-4372-9cb7-e75febb0dea1",
   "metadata": {},
   "source": [
    "## Exemple d'utilisation TT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da3cc47-55f4-428a-9909-2611bddabb5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_dataframe(\"data/echantillon_1000_TT_accords.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317fe23d-81ed-4ee0-8966-dfbfb7700aca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ngram_thresholds_tt_relative = {\n",
    "    'TT': [\n",
    "        (('jour', 'teletravail'), 0.001),\n",
    "        (('salar', 'situat', 'teletravail'), 0.0005)\n",
    "    ]\n",
    "}\n",
    "\n",
    "ngram_thresholds_tt_abs = {\n",
    "    'TT': [\n",
    "        (('jour', 'teletravail'), 3),\n",
    "        (('salar', 'situat', 'teletravail'), 2)\n",
    "    ]\n",
    "} \n",
    "\n",
    "classification(\"data/echantillon_hs_tt_cet_accords_et_thematiques.parquet\", 'TT',ngram_thresholds_tt_relative,ngram_thresholds_tt_abs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "my-uv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
