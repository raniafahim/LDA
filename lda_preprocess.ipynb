{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e6ea1f-4200-409b-aabe-9ca9fbf768ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!uv pip install -q nltk gensim pyLDAvis unidecode matplotlib seaborn pandas pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aec5d9-6ec6-49e9-8f52-6f032fe7ad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -q langchain-huggingface==0.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce00eec0-3fa4-4e19-af3d-c9aef10759a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pathlib import Path\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import html \n",
    "import json\n",
    "import matplotlib.pyplot  as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import unidecode\n",
    "import re\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import string\n",
    "import unidecode\n",
    "import warnings\n",
    "import torch\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac98aff4-e3dd-4ce9-8650-cbfdf63c7b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedLemmatizer:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.cache = {}  # Manual cache as a dictionary\n",
    "\n",
    "    def lemmatize(self, word, pos='n'):\n",
    "        if word in self.cache:\n",
    "            return self.cache[word]\n",
    "        else:\n",
    "            lemmatized_word = self.lemmatizer.lemmatize(word, pos)\n",
    "            self.cache[word] = lemmatized_word  # Store in cache\n",
    "            return lemmatized_word\n",
    "\n",
    "\n",
    "cached_lemmatizer = CachedLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7057d095-7e6a-452a-8047-0aee219cb911",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (torch.cuda.is_available()):\n",
    "    DEVICE=\"cuda\"\n",
    "else:\n",
    "    DEVICE=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513b3d25-b894-4b7c-a99d-dee825e9e41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25aa7df-d0b9-49ea-b9ad-3676c03c05ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR=\"intermediate_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5844e2-3cd7-4f72-b867-b2d863b4971a",
   "metadata": {},
   "source": [
    "# Utils preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0a2f1a-8226-414d-8769-07ee73cbd811",
   "metadata": {},
   "outputs": [],
   "source": [
    "DICTIONNARY =  ['accord','entreprise', 'preambule', 'sommaire',  'code', 'syndical', 'responsable', 'representant', \n",
    "                'present', 'ca', 'organisation', 'preambule', 'peut', 'etre', 'contrat','travail', 'ressources','humaines', 'mise',\n",
    "                'ainsi', 'et', 'ou', 'alors','collaborateur', 'ci', 'apres', 'party', 'signataire', 'tout', 'etat', 'cause', 'societe', \n",
    "                'notamment','article','activite', 'cette', 'donc', 'si', 'sous', 'disposition', 'convention', 'collective', 'dans', 'a', 'cadre',\n",
    "                'signataire', 'partie', 'parties', 'entre', 'doit', 'mme', 'mr', 'madame', 'monsieur'\n",
    "               ]\n",
    "\n",
    "DICTIONNARY_STEM = ['part', 'signatair', 'organis', 'syndical', \n",
    "                    'dont', 'sieg', 'social', 'conseil', 'prud', 'homm', \n",
    "                   'vi', 'professionnel', 'disposit', 'legal', 'conventionnel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac6426-07d7-4ca9-9766-7167511fd0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, lang=\"french\"):\n",
    "    # décoage HTML\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # nettoyage de tous les cractères spéciaux\n",
    "    text = re.sub(r\"&[a-z]+;\", \" \", text)\n",
    "    text = re.sub(r\"&#\\d+;\", \" \", text)\n",
    "    text = re.sub(r\"[<>{}\\[\\]\\|\\^\\~`\\\"'=]+\", \" \", text)\n",
    "    text = re.sub(r\"[–—•«»]+\", \" \", text)  # Tirets longs, puces, guillemets français\n",
    "\n",
    "    # tokenisation\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # lemming\n",
    "    #stemmer = SnowballStemmer(lang)\n",
    "                  \n",
    "    wnl = cached_lemmatizer\n",
    "   \n",
    "    words_cleaned = []\n",
    "    for w in words:\n",
    "        #w_norm = unidecode.unidecode(w.lower())\n",
    "        w_norm = w.lower()\n",
    "        if (\n",
    "            w_norm not in stop_words\n",
    "            and w_norm not in DICTIONNARY\n",
    "            and w_norm not in string.punctuation\n",
    "            and not re.search(r\"[<>]|--+|__+|xx+|==+\", w_norm)\n",
    "            and not w_norm.isnumeric()\n",
    "            and len(w_norm) > 2\n",
    "        ):\n",
    "            words_cleaned.append(wnl.lemmatize(w_norm))\n",
    "            #words_cleaned.append(stemmer.stem(w_norm))\n",
    "\n",
    "    return words_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38e64c9-9220-40dd-9c3b-9619d04c355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    return unidecode.unidecode(text.lower().strip())\n",
    "\n",
    "def split_text_by_sentences(text, flagged_sentences):\n",
    "    \"\"\"\n",
    "    Découpe le texte en segments basés sur les titres du sommaire, après normalisation.\n",
    "    \"\"\"\n",
    "    split_texts = []\n",
    "    positions = []\n",
    "\n",
    "    normalized_text = normalize(text)\n",
    "\n",
    "    # On garde un mapping (titre original, position) pour préserver les titres initiaux\n",
    "    for sentence in flagged_sentences:\n",
    "        norm_sentence = normalize(sentence)\n",
    "        pos = normalized_text.find(norm_sentence)\n",
    "        if pos != -1:\n",
    "            # On retrouve la position réelle dans le texte original\n",
    "            real_pos = text.lower().find(sentence.lower())\n",
    "            if real_pos != -1:\n",
    "                positions.append(real_pos)\n",
    "\n",
    "    # Si aucune position trouvée, retourner le texte complet\n",
    "    if not positions:\n",
    "        return [text]\n",
    "\n",
    "    positions = sorted(set(positions))\n",
    "    positions.insert(0, 0)\n",
    "    positions.append(len(text))\n",
    "\n",
    "    for i in range(len(positions) - 1):\n",
    "        start = positions[i]\n",
    "        end = positions[i + 1]\n",
    "        split_texts.append(text[start:end].strip())\n",
    "\n",
    "    return split_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a62d37-f783-4eba-8865-7be84570e5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Révision-- de l&rsquo;accord : &gt;&gt;' Tous les deux ans, les partenaires sociaux se réunissent. << Suivi de l’accord.\"\n",
    "print(preprocess_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db94316-bff0-439c-a1b3-025ad4324336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_with_titles(text, summary_titles):\n",
    "    chunks = split_text_by_sentences(text, summary_titles)\n",
    "    result = {}\n",
    "    for title in summary_titles:\n",
    "        for chunk in chunks:\n",
    "            if normalize(title) in normalize(chunk[:len(title)+30]):\n",
    "                result[title] = chunk.strip()\n",
    "                break\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dcf11e-dc9d-4b74-869f-3e1e2afaa07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {'device': DEVICE}  \n",
    "MODEL_NAME_EMBEDDER=\"BAAI/bge-small-en-v1.5\"  #petit modèle en anglais\n",
    "#MODEL_NAME_EMBEDDER=\"BAAI/bge-m3\" #gros modèle multilingue\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(\n",
    "    model_name=MODEL_NAME_EMBEDDER, \n",
    "    model_kwargs=model_kwargs,\n",
    "    show_progress=False\n",
    ")\n",
    "\n",
    "\n",
    "phrases_non_metier = [\n",
    "    \"Révision de l’accord\",\n",
    "    \"Dénonciation de l’accord\",\n",
    "    \"Interprétation de l’accord\",\n",
    "    \"Suivi de l’accord\",\n",
    "    \"Durée de l’accord\",\n",
    "    \"Formalités de publicité et de dépôt\",\n",
    "    \"Publicité et dépôt\",\n",
    "    \"Date d'effet et durée\",\n",
    "    \"Champ d'application\",\n",
    "    \"Clause de revoyure\", \n",
    "    \"Information des représentants du personnel\", \n",
    "    \"Dispositions relatives à l’accord\",\n",
    "    \"Champ d’application\",\n",
    "    \"Commission de suivi\", \n",
    "    \"Pause déjeuner du personnel\", \n",
    "    \"Modification de l'accord\",\n",
    "    \"Adhésion\"\n",
    "    \n",
    "]\n",
    "\n",
    "# Embeddings des phrases non-métier\n",
    "ref_embeddings = embedder.embed_documents(phrases_non_metier)\n",
    "\n",
    "def filtre_par_similarite(phrases, seuil=0.85):##torp long utiliser version vectoisée\n",
    "    results = []\n",
    "    for phrase in phrases:\n",
    "        emb = embedder.embed_query(phrase)\n",
    "        sims = cosine_similarity([emb], ref_embeddings)[0]\n",
    "        if max(sims) < seuil:\n",
    "            results.append(phrase) \n",
    "    return results\n",
    "\n",
    "def filtre_par_similarite_vectorise(phrases, seuil=0.85):\n",
    "    if not phrases:\n",
    "        return []\n",
    "\n",
    "    phrase_embeddings = embedder.embed_documents(phrases)  \n",
    "    sims = cosine_similarity(phrase_embeddings, ref_embeddings)\n",
    "\n",
    "    # On garde les phrases dont la similarité max avec une phrase non-métier est < seuil\n",
    "    keep_idx = np.max(sims, axis=1) < seuil\n",
    "    return [phrase for phrase, keep in zip(phrases, keep_idx) if keep]\n",
    "\n",
    "    \n",
    "def filtre_chunks_par_titre(section_dict, phrases_non_metier, seuil=0.85): #seuil arbitraire : en tester plsr\n",
    "    \"\"\"\n",
    "    Ne garde que les chunks dont le titre est peu similaire aux phrases non métier.\n",
    "    \"\"\"\n",
    "    if not section_dict:\n",
    "        return []\n",
    "\n",
    "    titres = list(section_dict.keys())\n",
    "    chunks = list(section_dict.values())\n",
    "\n",
    "    # Embeddings des titres de section\n",
    "    titre_embeddings = embedder.embed_documents(titres)\n",
    "    ref_embeddings = embedder.embed_documents(phrases_non_metier)\n",
    "\n",
    "    sims = cosine_similarity(titre_embeddings, ref_embeddings)\n",
    "\n",
    "    # On garde les chunks dont le titre est peu similaire aux phrases non métier\n",
    "    keep_idx = np.max(sims, axis=1) < seuil\n",
    "    return [chunk.strip() for chunk, keep in zip(chunks, keep_idx) if keep]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4482d905-e003-40e2-a4ae-ebd659b79bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_1 =\"Publicité et dépôt\"\n",
    "phase_2='ARTICLE 7 - PUBLICITE ET DEPOT'\n",
    "\n",
    "phase_2_embeddings=embedder.embed_documents(phase_2)\n",
    "\n",
    "sims = cosine_similarity(ref_embeddings, phase_2_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc9b346-ff18-4e1b-8329-8267e4d6aa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtre_par_similarite_vectorise([\"Article 8 – Révision de l’accord\", 'Article 5: Contingent annuel d’heures supplémentaires']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a87ac56-c1df-4bb9-aeb4-8f7cbc0d7523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pas parfait \n",
    "filtre_par_similarite_vectorise([\"Article 8 – Révision de l’accord\", 'Article 5: Contingent annuel d’heures supplémentaires', 'Article 4 – Information du Comité Social et Economique', 'Article 5 - Dispositions relatives à l’accord']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae27aae-f103-429f-b2c9-414b5d2ff3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_chunks_filtered(section_dict, skip_titles=[\"préambule\", \"annexe\"], seuil_sim=0.85):\n",
    "    skip_titles_norm = [normalize(t) for t in skip_titles]\n",
    "\n",
    "    # supprimer le préambule et avant \n",
    "    titles = list(section_dict.keys())\n",
    "    preamble_idx = next((i for i, t in enumerate(titles) if \"préambule\" in normalize(t)), -1)\n",
    "    if preamble_idx != -1:\n",
    "        titles = titles[preamble_idx + 1:]\n",
    "\n",
    "    # garder les titres valides uniquement\n",
    "    valid_titles = [\n",
    "        t for t in titles if all(skip_kw not in normalize(t) for skip_kw in skip_titles_norm)\n",
    "    ]\n",
    "    candidate_dict = {t: section_dict[t] for t in valid_titles}\n",
    "\n",
    "    # filtrer par similarité des titres\n",
    "    return filtre_chunks_par_titre(candidate_dict, phrases_non_metier, seuil=seuil_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281ddc0-129d-415d-859b-1a3ec1dc124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mots_cles_non_metier = [\n",
    "    \"modification\", \"publicité\", \"dépôt\", \"champ d'application\", \"durée\", \n",
    "    \"revoyure\", \"révision\", \"suivi\", \"commission\", \"clause\", \n",
    "    \"formalité\", \"interprétation\", \"information\"\n",
    "]\n",
    "\n",
    "def titre_est_administratif(titre):\n",
    "    titre_clean = normalize(titre)\n",
    "    return any(mot in titre_clean for mot in mots_cles_non_metier)\n",
    "\n",
    "def get_valid_chunks_filtered_bis(section_dict, skip_titles=[\"préambule\", \"annexe\"], seuil_sim=0.85):\n",
    "    skip_titles_norm = [normalize(t) for t in skip_titles]\n",
    "\n",
    "    # Extraire tous les titres\n",
    "    all_titles = list(section_dict.keys())\n",
    "\n",
    "    # Chercher l'index du préambule\n",
    "    preamble_idx = next((i for i, t in enumerate(all_titles) if \"préambule\" in normalize(t)), -1)\n",
    "\n",
    "    # Garder seulement les sections à partir du préambule (exclut tout ce qui est avant)\n",
    "    if preamble_idx != -1:\n",
    "        filtered_section_dict = {t: section_dict[t] for t in all_titles[preamble_idx:]}\n",
    "    else:\n",
    "        filtered_section_dict = section_dict  # Si pas de préambule, on garde tout\n",
    "\n",
    "    # Garder les titres valides (pas dans les titres à ignorer)\n",
    "    valid_titles = [\n",
    "        t for t in filtered_section_dict.keys()\n",
    "        if all(skip_kw not in normalize(t) for skip_kw in skip_titles_norm)\n",
    "    ]\n",
    "    candidate_dict = {t: filtered_section_dict[t] for t in valid_titles}\n",
    "\n",
    "    if not candidate_dict:\n",
    "        return []\n",
    "\n",
    "    # Embeddings des titres\n",
    "    titres = list(candidate_dict.keys())\n",
    "    chunks = list(candidate_dict.values())\n",
    "\n",
    "    titre_embeddings = embedder.embed_documents(titres)\n",
    "    ref_embeddings = embedder.embed_documents(phrases_non_metier)\n",
    "\n",
    "    sims = cosine_similarity(titre_embeddings, ref_embeddings)\n",
    "    keep_sim = np.max(sims, axis=1) < seuil_sim\n",
    "\n",
    "    # Filtrage combiné : similarité + heuristique\n",
    "    results = [\n",
    "        chunk.strip()\n",
    "        for titre, chunk, keep in zip(titres, chunks, keep_sim)\n",
    "        if keep and not titre_est_administratif(titre)\n",
    "    ]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626eb3f0-b066-4f48-9e7f-dbc8a2f3b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## exemple d'utilisaton \n",
    "\n",
    "mon_dict ={'Préambule :  ': 'Préambule :  \\n\\nConformément aux dispositions du code du travail, la Direction a invité le CSE, en l’absence d’organisations syndicales représentatives dans la structure  à participer à une négociation collective.\\n\\nAux termes des réunions en date des 10/10/2022, 16/11/2022 et 12/12/2022 ayant permis de rapprocher les points de vue de chacun, les parties ont abouti à la conclusion du présent accord.',\n",
    " 'Article 1 – Champ d’application et bénéficiaires\\xa0:  ': 'Article 1 – Champ d’application et bénéficiaires\\xa0:  \\n\\nLe présent accord concerne l’ensemble des établissements de l’HADVR.\\n\\nIl concerne tous les salariés quel que soit leur contrat (CDD ou CDI), quelle que soit leur durée de travail et quel que soit leur métier.\\nPar ailleurs, pour répondre aux aspirations des salariés d’une part et aux contraintes inhérentes d’une HAD, les parties se sont accordées pour poursuivre les négociations tout au long de l’année 2023 en vue de la conclusion éventuelle d’un accord sur l’aménagement du temps de travail au sein de la structure.',\n",
    " 'Article 2\\xa0: Rémunération et temps de travail': 'Article 2\\xa0: Rémunération et temps de travail',\n",
    " '2-1\\xa0: Prime de partage de la valeur': \"2-1\\xa0: Prime de partage de la valeur\\n\\nDe nombreux investissements matériels et humains ont été réalisés au cours de l’année 2022 pour répondre aux besoins de la structure. Ces investissements auront pour conséquence un budget 2022 non équilibré. \\nLe conseil d’administration de la structure, conscient des efforts des professionnels pour poursuivre la montée en charge du nombre de patients accueillis en HAD a répondu favorablement pour le versement d’une prime de partage de la valeur de 300 euros à l’ensemble du personnel excepté la Direction, dans les conditions énoncées ci-après.\\n\\nAfin de bénéficier des exonérations de cotisation sociales et de l’impôt sur le revenu, est éligible le personnel qui, à la date de versement de la prime, c’est-à-dire au 28 février 2023 :\\nLié par un contrat de travail ou d’apprentissage ;\\nTravailleurs handicapés liés par un contrat de soutien et d’aide par le travail à un ESAT\\xa0;\\nLes intérimaires ;\\nAyant une rémunération brute inférieure à 3 SMIC conformément aux dispositions légales au cours des 12 mois précédant la date de versement de la prime. \\n\\nLe salaire annuel brut s’entend de la rémunération annuelle brute (variable et primes inclus) reconstituée en équivalent temps plein sur la période allant de février 2022 à janvier 2023, soit 12 mois. \\nIl convient de préciser que la prime versée est calculée au prorata\\xa0de la durée de présence effective et du temps de travail contractuel sur la période précitée. \\nPar ailleurs et conformément aux dispositions légales, les absences pour congé de maternité, de paternité et d'accueil de l'enfant ou d'adoption, les absences pour congé parental d'éducation, pour enfant malade et pour congé de présence parentale, ainsi que les absences pour accident du travail et maladie professionnelle, sont assimilées à des périodes de présence effective et ne seront donc pas décomptées dans le calcul du temps de travail effectif. \\nLa prime sera versée en seule fois avec la paie du mois de février 2023 et figurera sur le bulletin de salaire du mois de versement.\\nLa prime ne se substituera à aucun des éléments de rémunération, ni à des augmentations salariales ou prime prévues par un accord, par contrat de travail ou usages en vigueur.\",\n",
    " '2-2\\xa0: Prime «\\xa0bas salaires\\xa0»\\xa0:': '2-2\\xa0: Prime «\\xa0bas salaires\\xa0»\\xa0:\\n\\nLe 28 juin 2022 le Ministre de la transformation et de la fonction publique a annoncé une hausse du point d’indice pour les trois versants de la fonction publique applicable en une fois dès le 1er juillet 2022. Les partenaires sociaux de la branche se sont réunis afin de transposer dans la CCN51 la revalorisation intervenue dans la fonction publique. A l’issue des différentes réunions de négociation qui se sont tenues, aucune organisation syndicale n’a été signataire des textes mis à la signature. La FEHAP a pris une recommandation patronale réévaluant la valeur du point dans la CCN51 en date du 23 novembre 2022. \\nDans le contexte inflationniste des derniers mois, compte tenu de la concurrence accrue avec le secteur public, des tensions en matière de recrutement et de la nécessité de fidélisation des professionnels, il est décidé de mettre en en place, par accord d’entreprise, une mesure ciblée pour les «\\xa0bas salaires\\xa0», en sus de l’augmentation de la valeur du point CCN51.\\n\\n\\nLe conseil d’administration de la structure, conscient que la revalorisation de la valeur du point conventionnel à effet rétroactif au 1er juillet 2022 ne bénéficiera pas au personnel dont le coefficient et donc la rémunération reste à la valeur du SMIC, a répondu favorablement pour le versement d’une prime de 150 euros brute exceptionnelle pour les personnels concernés par ces coefficients au prorata de leur temps de travail contractuel. Cette prime permettra de «\\xa0compenser\\xa0» la régularisation de la différence sur la valeur du point du 1er juillet au 31 décembre 2022 dont ils ne pourront bénéficier, et sera versée en une fois, en même temps que la régularisation de la valeur du point faite pour les autres membres du personnel sur la paie de janvier 2023.\\n\\nLes bénéficiaires de la mesure sont tous les professionnels qu’ils soient à temps complet ou à temps partiel, en contrat à durée indéterminée ou en contrat à durée déterminée, qui, au 1er juillet 2022, après application de la valeur du point résultant de la recommandation patronale FEHAP du 23 novembre 2022, sont concernés par l’application de l’article 08-02 de la CCN51 relatif au salaire minimum conventionnel.\\n\\nCette prime est exclue de l’assiette de calcul de toutes les primes et indemnités prévues par la Convention Collective nationale du 31 octobre 1951.',\n",
    " '2-3\\xa0: Récupération des heures de fériés et fixation du jour de solidarité pour 2023\\xa0:': '2-3\\xa0: Récupération des heures de fériés et fixation du jour de solidarité pour 2023\\xa0:\\n\\nLa recommandation patronale du 4 septembre 2012\\xa0de la CCN51 avait créé 2 catégories de personnel concernant l’avantage du férié récupéré\\xa0: le personnel présent au 1er décembre 2011 ayant pu continuer à bénéficier des anciennes dispositions de la convention du fait d’avantages individuels acquis, et le personnel arrivé après le 1er décembre 2011 qui a dû se voir attribuer les nouveaux critères prévus dans la recommandation patronale. Cela a engendré un souci d’équité.\\n\\nA compter du 1er janvier 2023, tous les salariés, sans condition d’ancienneté, récupéreront les heures de fériés qu’elles soient travaillées ou non selon les modalités prévues dans la recommandation patronale du 4 septembre 2012.\\n\\nAu 1er janvier 2023, le don de la journée de solidarité se fera par le biais de la suppression d’une récupération de jour férié (hormis celle due au titre du 1er mai éventuellement générée).\\nSi le salarié apporte la preuve (bulletin de salaire faisant mention, attestation, …) qu’il a déjà effectué sous quelque forme que ce soit la journée solidarité au titre de l’année concernée auprès d’un autre employeur, il n’aura pas à l’effectuer au sein de la structure.\\nLe salarié ayant plusieurs employeurs effectue sa journée de solidarité chez chacun d’eux au prorata de sa durée contractuelle de travail, de ce fait si le salarié apporte la preuve (bulletin de salaire faisant mention, attestation, …) qu’il a effectué au prorata sa journée ou son don pour la journée solidarité, il ne l’effectuera qu’au prorata au sein de la structure.\\nLa journée de solidarité sera évoquée sur le bulletin de salaire de manière à pouvoir apporter la preuve qu’elle a été effectuée dans la structure.\\nCas du salarié qui n’a pas pu obtenir de récupération de férié\\xa0(pas de férié tombant sur un repos, suspension de contrat ou congé payé durant un férié)\\xa0: celui-ci donnera un RTT s’il est concerné par ce dispositif. S’il n’en a pas, il pourra donner un repos conventionnel (tel qu’un repos compensateur de nuit par exemple), sinon il effectuera 7 heures supplémentaires (ou moins selon son temps contractuel) selon les modalités à convenir avec son supérieur hiérarchique de manière à valider son don pour la journée de solidarité.',\n",
    " '2-4 : Revalorisation des heures supplémentaires\\xa0:': '2-4 : Revalorisation des heures supplémentaires\\xa0:\\n\\nAfin de récompenser les salariés qui accepteraient de remplacer un collègue absent au «\\xa0pied levé\\xa0», les parties ont convenu de valoriser les heures supplémentaires à hauteur de 150% au lieu de 125% pour toute demande effectuée le vendredi pour le week-end et le lundi, et 24h avant en semaine.',\n",
    " '2-5\\xa0: Prime parrainage\\xa0:': '2-5\\xa0: Prime parrainage\\xa0:\\n\\nLa prime de parrainage accordée en 2022 pour toute aide au recrutement de la part des salariés par présentation d’un candidat n’est pas reconduite pour l’année 2023.\\nToutefois, une prime de parrainage de 2\\xa0500€ brut, est accordée pour toute aide au recrutement d’un médecin praticien d’HAD (0,80 à 1 ETP) et versée à la fin de la période d’essai du professionnel.',\n",
    " 'Article 3\\xa0: conditions de travail': 'Article 3\\xa0: conditions de travail\\n\\n3-1\\xa0: Casiers nominatifs sur chaque antenne\\xa0:\\n\\nDe nouvelles antennes et locaux ont été aménagés en 2022. Pour répondre à la problématique d’accueil de nouveaux collaborateurs et le travail en mobilité sur plusieurs antennes, les parties se sont accordées sur l’agencement de bureaux partagés nécessitant la mise à disposition de casiers nominatifs au sein de chaque antenne.\\nLa direction s’engage à réaliser les achats nécessaires pour la mise à disposition de ces casiers nominatifs au sein de chaque antenne dès l’agencement terminé.',\n",
    " '3-2\\xa0: pause déjeuner du personnel\\xa0:': '3-2\\xa0: pause déjeuner du personnel\\xa0:\\n\\nLa demande des salariés est de réduire le temps de présence journalier sur leur lieu de travail et de diminuer le temps accordé à la pause repas à 30 mn au lieu d’une heure.\\n\\nLes parties s’accordent sur une pause de 30 mn à condition que cela n’affecte pas le fonctionnement du service. Les horaires de travail seront ajustés par les responsables en fonction de l’amplitude de la pause repas et devront correspondre aux besoins de l’établissement.',\n",
    " '3-3\\xa0: utilisation voitures de service\\xa0:': '3-3\\xa0: utilisation voitures de service\\xa0:\\n\\nLe personnel soignant pourra garder le véhicule de service en cas de travail sur 2 jours consécutifs, par nécessité de service. En contrepartie, le salarié s’engage, par tout moyen, à restituer le véhicule de service en cas d’absence non programmée. Cf modalités dans le règlement intérieur des véhicules de service signé par le personnel avec attestation de remisage.',\n",
    " 'Article 4 – Information du Comité Social et Economique': 'Article 4 – Information du Comité Social et Economique\\nLe CSE sera informé du présent accord lors de réunion du 19 janvier 2023, dans le cadre de sa mission au titre de l’article L2312-8 du code du travail.',\n",
    " 'Article 5 - Dispositions relatives à l’accord ': 'Article 5 - Dispositions relatives à l’accord \\nLe présent accord entre en application après son dépôt sur la plateforme de téléprocédure en application des conditions légales et réglementaires en vigueur, pour une durée indéterminée.\\nLe présent accord est également déposé au secrétariat-greffe du Conseil des Prud’hommes de Libourne.\\nIl pourra être révisé conformément aux dispositions légales.\\nIl fait l’objet des mesures de publicité prévues par les dispositions légales et réglementaires sur les lieux d’affichage habituels.\\n\\nFait à Libourne, le 19 janvier 2023, \\n\\n\\nSignature de la Direction\\xa0:\\n\\n\\nSignatures des membres titulaires du CSE\\xa0:'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd122a1-f3f9-49c9-a106-264dd0da0a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(get_valid_chunks_filtered_bis(mon_dict, skip_titles=[\"préambule\", \"annexe\"], seuil_sim=0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034efc36-5f75-40c1-a0c0-800e43257d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(get_valid_chunks_filtered(mon_dict, skip_titles=[\"préambule\", \"annexe\"], seuil_sim=0.85))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eef58e-ff2f-46d4-a5a2-39685c6e1a9e",
   "metadata": {},
   "source": [
    "# Pour HS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c835a3-733e-4935-aead-ecb75e2359b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sommaire_hs = pd.read_parquet(\"data/echantillon_1000_hs_accords_TOC.parquet\")\n",
    "df_hs = pd.read_parquet(\"data/echantillon_1000_hs_accords.parquet\")\n",
    "df_hs = df_hs.set_index(\"numdossier_new\")\n",
    "df_hs = df_hs.merge(sommaire_hs,how=\"inner\",left_index=True,right_index=True)\n",
    "df_hs = df_hs.rename(columns={\"extracted_summary\":\"summary\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da5cd9a-2f5e-44b6-9fd7-883633872180",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hs[\"section_dict\"] = df_hs.apply(\n",
    "    lambda row: split_text_with_titles(row[\"accorddocx\"], row[\"summary\"]),\n",
    "    axis=1\n",
    ")\n",
    "#df_hs[\"section_dict\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c0f3c3-107a-4837-a55f-f2523106a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hs[\"lda_documents\"] = df_hs[\"section_dict\"].apply(get_valid_chunks_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72406b6a-514c-417d-8d01-0f944719e0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage NLP + lemming\n",
    "all_chunks_hs = [chunk for doc_chunks in df_hs[\"lda_documents\"] for chunk in doc_chunks]\n",
    "processed_texts_hs = [preprocess_text(doc) for doc in all_chunks_hs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d6f87c-342e-449b-8382-3e4e1ed2da23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed_texts_hs = pd.DataFrame({\"processed_texts_hs\": processed_texts_hs})\n",
    "df_processed_texts_hs.to_parquet(f\"{OUTPUT_DIR}/processed_texts_hs.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c816c28a-8a40-4b27-bf97-5fa8f3c677af",
   "metadata": {},
   "source": [
    "# Pour les données de santé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2b74d3-2cef-46cb-b739-c481389e4316",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sante= pd.read_parquet(\"data/complementaire_sante_580.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951b2e23-a965-4c2b-b8b5-c85834eba868",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sante[\"section_dict\"] = df_sante.apply(\n",
    "    lambda row: split_text_with_titles(row[\"accorddocx\"], row[\"extracted_summary\"]),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae8b36-3c51-4b59-becc-712bf0654dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sante[\"lda_documents\"] = df_sante[\"section_dict\"].apply(get_valid_chunks_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f379b5b-7b71-455a-984c-c743f5cca718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage NLP + lemming\n",
    "all_chunks_sante = [chunk for doc_chunks in df_sante[\"lda_documents\"] for chunk in doc_chunks]\n",
    "processed_texts_sante = [preprocess_text(doc) for doc in all_chunks_sante]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bedac0-cc46-4bc2-8d7b-a87e41ad4d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed_texts_sante = pd.DataFrame({\"processed_texts_sante\": processed_texts_sante})\n",
    "df_processed_texts_sante.to_parquet(f\"{OUTPUT_DIR}/processed_texts_sante.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "my-uv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
